# Tox configuration for mini-biai-1 testing across multiple environments
# =================================================================

[tox]
# Minimum tox version
minversion = 3.8.0

# Python versions to test against
envlist = 
    py38
    py39
    py310
    py311
    lint
    typecheck
    format
    security
    coverage
    docs
    performance

# Skip missing interpreters
skip_missing_interpreters = true

# Parallel execution settings
parallel_show_output = true

[testenv]
# Base test environment configuration
deps =
    pytest>=7.0.0
    pytest-cov>=4.0.0
    pytest-mock>=3.10.0
    pytest-asyncio>=0.21.0
    pytest-benchmark>=4.0.0
    pytest-xdist>=3.0.0
    pytest-timeout>=2.1.0
    responses>=0.23.0
    factory-boy>=3.2.0
    freezegun>=1.2.0
    parameterized>=0.9.0
    hypothesis>=6.0.0
    faker>=18.0.0
    memory-profiler>=0.60.0
    psutil>=5.9.0
    numpy>=1.21.0
    scipy>=1.9.0
    torch>=1.13.0
    faiss-cpu>=1.7.0

# Install current package in development mode
usedevelop = true

# Commands to run tests
commands =
    # Run unit tests with coverage
    pytest {posargs:tests/} -m "unit" --cov=src --cov-report=term-missing --cov-fail-under=80

# Environment variables
setenv =
    PYTHONPATH = {toxinidir}/src
    COVERAGE_FILE = {toxworkdir}/.coverage.{envname}
    TESTING = true

# Pass through environment variables
passenv = 
    HOME
    USER
    PATH
    CUDA_VISIBLE_DEVICES
    PYTHONPATH
    CI
    GITHUB_*

# Test data directory
changedir = {toxinidir}

[testenv:integration]
# Integration tests environment
deps = {[testenv]deps}

commands =
    # Run integration tests
    pytest {posargs:tests/} -m "integration" --cov=src --cov-report=term-missing

[testenv:performance]
# Performance tests environment
deps = 
    {[testenv]deps}
    memory-profiler
    psutil

commands =
    # Run performance tests with benchmarking
    pytest {posargs:tests/} -m "performance" --benchmark-only

[testenv:lint]
# Linting environment
deps =
    flake8>=5.0.0
    flake8-bugbear>=22.0.0
    flake8-comprehensions>=3.10.0
    flake8-docstrings>=1.6.0
    flake8-import-order>=0.18.1
    flake8-simplify>=0.19.0

commands =
    # Run flake8 linting
    flake8 src/ tests/ scripts/
    # Check for common issues
    python -c "
    import ast
    import sys
    import os
    from pathlib import Path
    
    # Check for debug/print statements
    for file in Path('src').rglob('*.py'):
        with open(file) as f:
            lines = f.readlines()
        for i, line in enumerate(lines, 1):
            if 'print(' in line and not line.strip().startswith('#'):
                print(f'{file}:{i}: Found print statement: {line.strip()}')
                sys.exit(1)
    
    print('Lint checks passed!')
    "

[testenv:format]
# Code formatting environment
deps =
    black>=22.0.0
    isort>=5.10.0

commands =
    # Check black formatting
    black --check --diff src/ tests/ scripts/
    
    # Check import sorting
    isort --check-only --diff src/ tests/ scripts/
    
    # Run format verification
    python -c "
    import subprocess
    import sys
    
    # Verify no syntax errors
    for file in ['src', 'tests', 'scripts']:
        try:
            subprocess.run(['python', '-m', 'py_compile', file], check=True, capture_output=True)
            print(f'✓ {file} has no syntax errors')
        except subprocess.CalledProcessError as e:
            print(f'✗ Syntax error in {file}: {e}')
            sys.exit(1)
    "

[testenv:typecheck]
# Type checking environment
deps =
    mypy>=0.991
    types-requests
    types-PyYAML

commands =
    # Run mypy type checking
    mypy src/ --show-error-codes --show-column-numbers

    # Run additional type checks
    python -c "
    import ast
    import sys
    from pathlib import Path
    
    def check_type_annotations(file_path):
        with open(file_path) as f:
            try:
                tree = ast.parse(f.read())
            except SyntaxError:
                return
                
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                if not node.returns and not node.decorator_list:
                    # Function without return type annotation
                    if any( decorator.id == 'property' for decorator in node.decorator_list if isinstance(decorator, ast.Name)):
                        continue  # Skip properties
                    if len(node.args.args) > 0 and node.args.args[0].arg == 'self':
                        continue  # Skip methods
                    
                    # Check if function has parameters that should be typed
                    has_complex_params = any(
                        arg.annotation for arg in node.args.args if arg.annotation
                    )
                    
                    if has_complex_params:
                        print(f'{file_path}:{node.lineno}: Function {node.name} has typed parameters but no return type')
    
    for py_file in Path('src').rglob('*.py'):
        if '__pycache__' in str(py_file):
            continue
        check_type_annotations(py_file)
    "

[testenv:security]
# Security checking environment
deps =
    bandit[toml]>=1.7.0
    safety>=2.0.0

commands =
    # Run bandit security checks
    bandit -r src/ -f json -o bandit-security-report.json
    bandit -r src/ -f txt
    
    # Run safety dependency check
    safety check --json --output safety-report.json
    safety check

[testenv:coverage]
# Coverage reporting environment
deps =
    {[testenv]deps}
    coverage[toml]>=6.0

commands =
    # Run tests with comprehensive coverage
    pytest tests/ --cov=src --cov-report=term-missing --cov-report=html:htmlcov --cov-report=xml:coverage.xml --cov-fail-under=80 --cov-branch
    
    # Generate coverage report
    coverage report --show-missing --skip-covered --sort=Cover
    
    # Check coverage for individual modules
    coverage report --include="src/*" --show-missing

[testenv:docs]
# Documentation testing environment
deps =
    {[testenv]deps}
    sphinx>=4.0.0
    sphinx-rtd-theme>=0.5.0
    sphinx-autodoc-typehints>=1.12.0

commands =
    # Test docstring examples
    python -c "
    import doctest
    import importlib
    import sys
    from pathlib import Path
    
    # Find all Python modules
    modules = []
    for py_file in Path('src').rglob('*.py'):
        if '__pycache__' not in str(py_file):
            modules.append(py_file)
    
    # Run doctests
    for module_path in modules:
        module_name = str(module_path.relative_to('src')).replace('/', '.').replace('.py', '')
        try:
            module = importlib.import_module(module_name)
            doctest.testmod(module, verbose=False)
            print(f'✓ Doctests passed for {module_name}')
        except Exception as e:
            print(f'✗ Doctest failed for {module_name}: {e}')
    
    print('All docstring examples tested!')
    "

[testenv:benchmark]
# Benchmark testing environment
deps =
    {[testenv]deps}
    pytest-benchmark[psutil]>=4.0.0
    memory-profiler>=0.60.0
    psutil>=5.9.0

commands =
    # Run benchmarks with performance profiling
    pytest tests/ -m "performance" --benchmark-only --benchmark-json=benchmark-results.json
    
    # Run memory profiling
    python -c "
    import subprocess
    import sys
    import json
    from pathlib import Path
    
    print('Running memory profiling on critical components...')
    
    # Profile FAISS operations
    try:
        subprocess.run([
            'python', '-m', 'memory_profiler', 
            '-o', 'memory-profile-faiss.txt',
            'python', '-c', '''
import time
import numpy as np
from unittest.mock import MagicMock

# Simulate FAISS operations
for i in range(100):
    data = np.random.random((1000, 512)).astype('float32')
    time.sleep(0.001)
'''
        ], check=True)
        print('✓ Memory profiling completed')
    except Exception as e:
        print(f'✗ Memory profiling failed: {e}')
    "

[testenv:clean]
# Clean build artifacts
deps = 
skip_install = true

commands =
    python -c "
    import shutil
    import os
    from pathlib import Path
    
    # Clean common build artifacts
    patterns = [
        'build',
        'dist',
        '*.egg-info',
        '.tox',
        '__pycache__',
        '.pytest_cache',
        '.coverage',
        '.mypy_cache',
        'htmlcov',
        '*.pyc',
        '*.pyo',
    ]
    
    for pattern in patterns:
        for path in Path('.').glob(pattern):
            if path.is_dir():
                shutil.rmtree(path, ignore_errors=True)
                print(f'Removed directory: {path}')
            else:
                path.unlink(missing_ok=True)
                print(f'Removed file: {path}')
    
    print('Cleanup completed!')
    "

# Test environments for specific CI/CD workflows
[testenv:ci-lint]
# CI environment for linting
deps = {[testenv:lint]deps}

commands =
    flake8 src/ tests/ scripts/ --exit-zero

[testenv:ci-tests]
# CI environment for tests
deps = {[testenv]deps}

commands =
    pytest {posargs:tests/} --cov=src --cov-report=xml --cov-fail-under=80 -m "not slow"

[testenv:pre-commit]
# Pre-commit hook testing
deps =
    pre-commit>=2.20.0
    black>=22.0.0
    isort>=5.10.0
    flake8>=5.0.0
    mypy>=0.991

commands =
    pre-commit run --all-files

# Test environment for debugging
[testenv:debug]
# Debug environment with additional tools
deps =
    {[testenv]deps}
    ipdb>=0.13.0
    ipython>=8.0.0
    pdbpp>=0.10.0

commands =
    # Run tests with debugging enabled
    pytest {posargs:tests/} --pdb-trace --verbose

# Environment configuration
[testenv:py38]
# Python 3.8 specific settings
basepython = python3.8

[testenv:py39]
# Python 3.9 specific settings  
basepython = python3.9

[testenv:py310]
# Python 3.10 specific settings
basepython = python3.10

[testenv:py311]
# Python 3.11 specific settings
basepython = python3.11

# Flake8 configuration
[flake8]
max-line-length = 88
exclude = 
    .git,
    __pycache__,
    .pytest_cache,
    .coverage,
    htmlcov,
    build,
    dist,
    *.egg-info,
    venv,
    .venv,
    env,
    .env,
    migrations

ignore = 
    E501,  # Line too long (handled by black)
    E203,  # Whitespace before ':'
    W503,  # Line break before binary operator
    W504   # Line break after binary operator

# Coverage configuration
[coverage:run]
source = src
branch = true
parallel = true
omit = 
    */tests/*
    */test_*
    */conftest.py
    */__pycache__/*
    */setup.py
    */migrations/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    class .*\bProtocol\):

[coverage:html]
directory = htmlcov

[coverage:xml]
output = coverage.xml