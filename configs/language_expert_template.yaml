# Language Expert Configuration Template
# Specialized configuration for language processing capabilities

language_expert:
  # Basic configuration inherited from step2_base.yaml
  inherit_from: "step2_base"
  
  # Expert identity and role
  expert_info:
    name: "Language Expert"
    domain: "text_processing"
    primary_functions:
      - "text_understanding"
      - "text_generation" 
      - "natural_language_reasoning"
      - "conversational_context"
    secondary_functions:
      - "language_translation"
      - "summarization"
      - "question_answering"
      
  # Architecture configuration
  architecture:
    type: "mamba"  # mamba, transformer, linear_attention
    d_model: 512
    n_layers: 4
    dropout: 0.1
    
    # Mamba-specific parameters
    mamba_config:
      d_state: 64
      d_conv: 4
      expand: 2
      dt_rank: "auto"
      
    # Transformer fallback
    transformer_config:
      n_heads: 8
      intermediate_size: 2048
      layer_norm_epsilon: 1e-6
      
  # Input processing
  input_processing:
    max_input_length: 2048
    tokenization:
      tokenizer: "gpt2_tokenizer"
      vocab_size: 50257
      special_tokens:
        bos_token: ""
        eos_token: ""
        pad_token: "<|pad|>"
        
    # Text preprocessing
    text_normalization: true
    lowercase: false  # Preserve case for language tasks
    remove_punctuation: false
    
  # Language-specific capabilities
  capabilities:
    # Text generation
    text_generation:
      enabled: true
      max_new_tokens: 512
      temperature: 0.7
      top_p: 0.9
      repetition_penalty: 1.1
      do_sample: true
      
    # Natural language understanding
    nlu:
      enabled: true
      tasks:
        - "classification"
        - "sentiment_analysis" 
        - "entity_recognition"
        - "question_answering"
        - "text_summarization"
        
    # Conversational context
    conversational_context:
      enabled: true
      context_window: 1024
      conversation_memory: true
      dialogue_state_tracking: false  # Will be enabled in Step 3
      
    # Multi-turn reasoning
    reasoning:
      enabled: true
      reasoning_depth: 3
      chain_of_thought: false  # Will be enabled in Step 3
      step_by_step: false
      
  # Performance parameters
  performance:
    # Latency requirements
    max_inference_time_ms: 50
    target_throughput_qps: 20
    
    # Accuracy requirements
    min_accuracy: 0.8
    evaluation_metrics:
      - "perplexity"
      - "bleu_score"
      - "rouge_score"
      - "accuracy"
      
  # Memory integration
  memory_integration:
    # Context retrieval
    context_retrieval:
      enabled: true
      top_k_chunks: 5
      chunk_size: 512
      overlap: 64
      
    # Expert-specific STM
    stm:
      expert_specific_buffer: true
      buffer_size: 1024
      language_context_weight: 1.0
      
  # Affect modulation integration
  affect_integration:
    # Language affects by affect state
    valence_sensitivity: 0.3  # How much valence affects language choice
    arousal_modulation: 0.2   # How arousal affects complexity
    emotion_consistency: true # Maintain emotional tone
    
    # Sentiment-aware processing
    sentiment_analysis:
      enabled: true
      emotion_categories: 8
      confidence_threshold: 0.7
      
  # Training configuration for language expert
  training:
    # Task-specific training
    tasks:
      - "language_modeling"
      - "text_classification" 
      - "text_summarization"
      - "question_answering"
      
    # Loss weights
    loss_weights:
      language_modeling: 0.4
      nlu_tasks: 0.3
      conversational: 0.2
      affect_prediction: 0.1
      
    # Optimization
    learning_rate: 5e-5
    weight_decay: 0.01
    adam_optimizer:
      beta1: 0.9
      beta2: 0.999
      epsilon: 1e-8
      
  # Evaluation configuration
  evaluation:
    datasets:
      - "wikitext-103"
      - "glue"
      - "squad"
      - "convai"
      
    metrics:
      perplexity:
        enabled: true
        stride: 512
        
      generation_quality:
        enabled: true
        metrics: ["bleu", "rouge", "meteor"]
        
      understanding_tasks:
        enabled: true
        accuracy_threshold: 0.8
        
  # Deployment configuration
  deployment:
    # Serving parameters
    serving:
      batch_size: 32
      max_sequence_length: 512
      timeout_ms: 5000
      
    # Caching
    caching:
      enabled: true
      cache_size: 1000
      cache_strategy: "lru"
      
  # Integration with routing system
  routing_integration:
    # Routing preferences
    preferred_tasks:
      - "text_generation"
      - "conversational_tasks"
      - "language_understanding"
      
    # Expertise levels
    expertise_levels:
      creative_writing: 0.9
      technical_writing: 0.8
      conversational: 0.9
      reasoning: 0.7
      factual: 0.8
      
  # Configuration overrides from base
  base_overrides:
    # Override specific settings from step2_base.yaml
    routing:
      experts:
        language:
          activation_threshold: 0.5
          capacity_weight: 1.0
          
    language_backbone:
      # Ensure language backbone is used for this expert
      type: "mamba"
      
  # Expert metadata
  metadata:
    version: "0.2.0"
    author: "mini-biai-1 Team"
    description: "Language expert for text processing and generation"
    capabilities_summary: |
      This expert specializes in all aspects of language processing including:
      - Text generation and understanding
      - Conversational AI and dialogue systems  
      - Natural language reasoning and analysis
      - Multi-modal text processing
      - Affect-aware language generation
      
  # Validation rules
  validation:
    required_fields:
      - "expert_info.name"
      - "architecture.type"
      - "capabilities.text_generation.enabled"
      - "performance.max_inference_time_ms"
      
    value_constraints:
      max_inference_time_ms: [10, 1000]
      min_accuracy: [0.0, 1.0]
      expertise_levels: [0.0, 1.0]