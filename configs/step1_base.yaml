# mini-biai-1 Configuration - Step 1 Base Settings
# This is the base configuration for the mini-biai-1 computational model

# General settings
general:
  project_name: "mini-biai-1"
  version: "0.1.0"
  debug: false
  random_seed: 42
  device: "auto"  # auto, cpu, cuda

# Data settings
data:
  batch_size: 32
  sequence_length: 512
  vocab_size: 30000
  max_position_embeddings: 1024
  
  # Data paths
  train_data_path: "data/train/"
  val_data_path: "data/val/"
  test_data_path: "data/test/"
  model_save_path: "models/"
  tensorboard_log_path: "logs/tensorboard/"
  
  # Preprocessing
  normalize: true
  padding_token_id: 0
  mask_token_id: 1
  special_tokens:
    - [PAD, 0]
    - [MASK, 1] 
    - [CLS, 2]
    - [SEP, 3]
    - [UNK, 4]

# Memory architecture settings
memory:
  # Hierarchical Memory Structure
  working_memory:
    capacity: 1000
    decay_rate: 0.01
    attention_heads: 8
    head_dim: 64
    
  episodic_memory:
    capacity: 10000
    indexing_method: "faiss"
    similarity_threshold: 0.85
    
  semantic_memory:
    vector_dim: 768
    embedding_model: "bert-base-uncased"
    update_frequency: 100
    
  # Memory consolidation
  consolidation:
    enabled: true
    interval: 1000  # steps
    strength_threshold: 0.7
    decay_factor: 0.1

# Spiking Neural Network settings
snn:
  # Neuron parameters
  threshold: 1.0
  reset_value: 0.0
  membrane_decay: 0.9
  spike_decay: 0.5
  
  # Network architecture
  num_layers: 6
  hidden_size: 512
  num_attention_heads: 8
  intermediate_size: 2048
  
  # Temporal dynamics
  time_steps: 8
  firing_rate_target: 0.1
  
  # Synaptic plasticity
  plasticity_enabled: true
  learning_rate: 0.001
  weight_decay: 0.01

# Language model settings
language:
  # Model architecture
  model_type: "spiking-transformer"
  vocab_size: 30000
  hidden_size: 512
  num_hidden_layers: 6
  num_attention_heads: 8
  intermediate_size: 2048
  max_position_embeddings: 1024
  
  # Attention mechanisms
  attention_probs_dropout_prob: 0.1
  hidden_dropout_prob: 0.1
  
  # Layer normalization
  layer_norm_epsilon: 1e-6
  
  # Initialization
  initializer_range: 0.02
  
  # Loss function
  loss_type: "cross_entropy"
  label_smoothing: 0.1

# Training settings
training:
  # Optimization
  optimizer: "adamw"
  learning_rate: 5e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  
  # Scheduler
  scheduler: "cosine"
  warmup_steps: 1000
  max_steps: 100000
  
  # Training loop
  max_epochs: 10
  eval_steps: 1000
  save_steps: 1000
  logging_steps: 100
  gradient_accumulation_steps: 1
  
  # Regularization
  max_grad_norm: 1.0
  dropout_rate: 0.1
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 3
    min_delta: 0.001

# Inference settings
inference:
  # Generation parameters
  temperature: 0.7
  top_k: 50
  top_p: 0.9
  repetition_penalty: 1.1
  max_length: 512
  
  # Beam search
  beam_size: 5
  length_penalty: 1.0
  
  # Sampling strategies
  sampling_strategy: "top_p"  # greedy, top_k, top_p, nucleus
  
  # Caching
  use_cache: true
  cache_size: 1000

# Evaluation metrics
evaluation:
  metrics:
    - "perplexity"
    - "bleu"
    - "rouge"
    - "accuracy"
    - "f1_score"
  
  # Perplexity calculation
  perplexity:
    stride: 512
    
  # Language evaluation
  language_eval:
    model_name: "gpt2"
    eval_dataset: "wikitext-103-raw-v1"

# Hardware specifications
hardware:
  # GPU settings
  gpu_memory_fraction: 0.8
  mixed_precision: true
  
  # Distributed training
  distributed:
    enabled: false
    world_size: 1
    rank: 0
    
  # Memory optimization
  memory_optimization:
    gradient_checkpointing: true
    cpu_offloading: false
    
# Logging and monitoring
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "logs/tensorboard/"
    
  # Weights and Biases
  wandb:
    enabled: false
    project_name: "mini-biai-1"
    
# Checkpoint settings
checkpointing:
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
# Reproducibility
reproducibility:
  deterministic: true
  benchmark: false
  
# Experimental features
experimental:
  neuromorphic_computing: true
  synaptic_plasticity: true
  meta_learning: false
  few_shot_learning: false
